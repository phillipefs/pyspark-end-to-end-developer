{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/29 06:54:15 WARN Utils: Your hostname, DESKTOP-0KANSSR resolves to a loopback address: 127.0.1.1; using 192.168.228.28 instead (on interface eth0)\n",
      "23/12/29 06:54:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/12/29 06:54:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"SQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import Row\n",
    "import random\n",
    "\n",
    "## Criando DataFrame de clientes\n",
    "schema_clientes = StructType([\n",
    "    StructField('cliente_id', IntegerType(), True),\n",
    "    StructField('nome', StringType(), True),\n",
    "    StructField('idade', IntegerType(), True),\n",
    "    StructField('cidade', StringType(), True),\n",
    "    StructField('genero', StringType(), True)\n",
    "])\n",
    "\n",
    "clientes_data = [\n",
    "    (i, 'Cliente' + str(i), random.randint(18, 65),\n",
    "     random.choice(['São Paulo', 'Rio de Janeiro', 'Belo Horizonte', 'Brasília']),\n",
    "     random.choice(['M', 'F']))\n",
    "    for i in range(1, 1000)\n",
    "]\n",
    "\n",
    "df_clientes = spark.createDataFrame(clientes_data, schema=schema_clientes)\n",
    "\n",
    "# Criando DataFrame de produtos\n",
    "schema_produtos = StructType([\n",
    "    StructField('produto_id', IntegerType(), True),\n",
    "    StructField('nome_produto', StringType(), True),\n",
    "    StructField('categoria', StringType(), True),\n",
    "    StructField('preco', FloatType(), True)\n",
    "   \n",
    "])\n",
    "\n",
    "produtos_data = [\n",
    "    Row(\n",
    "        produto_id=i,\n",
    "        nome_produto='Produto' + str(i),\n",
    "        categoria=random.choice(['Eletrônicos', 'Roupas', 'Alimentos', 'Acessórios']),\n",
    "        preco=random.uniform(10, 1000),  # Convertendo para float\n",
    "        disponibilidade=random.choice([True, False])\n",
    "    )\n",
    "    for i in range(1, 1000)\n",
    "]\n",
    "\n",
    "# Criando o DataFrame de produtos\n",
    "df_produtos = spark.createDataFrame(produtos_data)\n",
    "\n",
    "# # Criando DataFrame de vendas\n",
    "schema_vendas = StructType([\n",
    "    StructField('venda_id', IntegerType(), True),\n",
    "    StructField('cliente_id', IntegerType(), True),\n",
    "    StructField('produto_id', IntegerType(), True),\n",
    "    StructField('quantidade', IntegerType(), True),\n",
    "    StructField('data_venda', StringType(), True)\n",
    "])\n",
    "\n",
    "vendas_data = [\n",
    "    (i, random.randint(1, 100), random.randint(1, 50), random.randint(1, 5), '2023-01-01')\n",
    "    for i in range(1, 1000)\n",
    "]\n",
    "\n",
    "df_vendas = spark.createDataFrame(vendas_data, schema=schema_vendas)\n",
    "\n",
    "# Criando DataFrame de transações\n",
    "schema_transacoes = StructType([\n",
    "    StructField('transacao_id', IntegerType(), True),\n",
    "    StructField('produto_id', IntegerType(), True),\n",
    "    StructField('valor', FloatType(), True),\n",
    "    StructField('tipo', StringType(), True),\n",
    "    StructField('status', StringType(), True)\n",
    "])\n",
    "\n",
    "transacoes_data = [\n",
    "    (i, random.randint(1, 50), random.uniform(50, 500),\n",
    "     random.choice(['Débito', 'Crédito']), random.choice(['Aprovado', 'Pendente', 'Rejeitado']))\n",
    "    for i in range(1, 1000)\n",
    "]\n",
    "\n",
    "df_transacoes = spark.createDataFrame(transacoes_data, schema=schema_transacoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group by with SUM and Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   tipo|total|\n",
      "+-------+-----+\n",
      "|Crédito| 0.09|\n",
      "| Débito| 0.08|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transacoes.groupBy(\"tipo\")\\\n",
    "    .agg(\n",
    "        ((sum(when(col(\"status\") == \"Aprovado\", 1).otherwise(0)) / \n",
    "         sum(when(col(\"status\") == \"Pendente\", 1).otherwise(5)) * 100).cast(\"int\").alias(\"teste\") /100).alias(\"total\")\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group by with SUM and Case (Using EXPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   tipo|Teste|\n",
      "+-------+-----+\n",
      "|Crédito| 0.09|\n",
      "| Débito| 0.08|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transacoes.groupBy(\"tipo\").agg(\n",
    "    expr(\"CAST((sum(CASE WHEN status = 'Aprovado' THEN 1 ELSE 0 END) / sum(CASE WHEN status = 'Pendente' THEN 1 ELSE 5 END) * 100) AS INT) / 100 as Teste\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join with where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+----------+----------+------------+---------+----------------+---------------+\n",
      "|produto_id|venda_id|cliente_id|quantidade|data_venda|nome_produto|categoria|           preco|disponibilidade|\n",
      "+----------+--------+----------+----------+----------+------------+---------+----------------+---------------+\n",
      "|        29|     990|        63|         4|2023-01-01|   Produto29|Alimentos|857.759464844174|           true|\n",
      "|        29|     923|        20|         2|2023-01-01|   Produto29|Alimentos|857.759464844174|           true|\n",
      "|        29|     803|        62|         3|2023-01-01|   Produto29|Alimentos|857.759464844174|           true|\n",
      "+----------+--------+----------+----------+----------+------------+---------+----------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vendas.alias(\"v\")\\\n",
    "    .join(df_produtos.alias(\"p\").filter(\"categoria = 'Alimentos'\"), \"produto_id\")\\\n",
    "    .show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
